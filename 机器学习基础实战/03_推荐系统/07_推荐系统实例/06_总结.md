### 数据集分析

- 召回

  - 采用用户的行为日志 behavior_log 创建召回模型
  - 协同过滤   **ALS**——Spark有封装
    - 协同过滤需需要用户对物品的评分。
    - 用户-物品 评分
      - 这里没有。只有对品类、品牌的数据。
      - 用户-品类 评分
      - 用户-品牌 评分
    - pv  cart fav buy通过四种行为转化成 -> 评分
      - 评分的设定是根据具体的业务来的。如买给的评分最高，看最低。以及不同行为的上限，看了100次，那最多评分50.

- 排序

  - LR 逻辑回归
  - 以  **raw_sample**  为骨架 把ad_feature 广告信息和user_profile用户信息拼接过来 训练逻辑回归模型
  - 训练逻辑回归模型时 ：
    - 用到user_profile中会影响到用户是否会点击广告的用户特征 
    - 用到ad_feature会影响到用户是否会点击广告的特征 
    - 点/不点作为目标值
- 预测的是点击的概率
    - 若预测是0,1的话全部都是0.（实际广告100个人，有5个人点，正负样本极度不均衡，那我模型直接预测成全部不点也很好。所以需要调整阈值，概率不能以0.5划分了。得到结果处理：如果几个物品不点的概率分别为，0.87 0.88 0.91），那我先推荐的是点的概率大的0.13那个物品。

  

  

  

### spark 训练 ALS 模型

- spark 机器学习相关的库

  - spark MLlib
    - 最早开发的
    - **基于RDD 的api**
    - 目前已经停止维护了 （从2.3开始停止维护）
    - 还可以使用
  - spark ML
    - 目前在更新的是这个库
    - **基于dataframe**

- ALS 模型训练

  - spark ML的库中封装了 协同过滤的 ALS模型

  - from pyspark.ml.recommendation import ALS

  - 需要准备一个dataframe 包含 用户id 物品id 用户-物品评分 这三列，利用这三列数据就可以使用spark ALS模块训练ALS模型

    ```python
    from pyspark.ml.recommendation import ALS
    als = ALS(userCol = 'userId',itemCol='cateId',ratingCol = 'rating',checkpointInterval = 5)
    model = als.fit(dataframe)
    ```

    

  - 训练出模型之后就可以为用户召回物品

    ```python
    model.recommendForAllUsers(3)
    #为指定用户推荐物品
    dataset = spark.createDataFrame([[1],[2],[3]])
    dataset = dataset.withColumnRenamed("_1", "userId")
    ret = model.recommendForUserSubset(dataset, 3)
    ```

### 缺失值处理

- 连续的特征
  - 缺失比例比较严重 可以考虑舍弃
  - 可以考虑使用平均值 中位数 分位数填充
  - 算法预测 （利用样本中的其它特征作为 特征值，有缺失的特征作为目标值）
- 分类的特征
  - 缺失比例比较严重 可以考虑舍弃
  - 把缺失作为单独的分类， 如果之前的数据只有两个分类，那么把缺失考虑进来就变成3个分类
  - 算法预测
- 利用算法预测缺失值
  - 其它特征和要预测的特征之间是否有联系
  - 样本数据是否足够
  - 利用算法预测缺失值会引入噪声

### 利用随机森林预测缺失值

- pyspark MLlib
  - 基于RDD的
  - 监督学习的样本数据要创建成LabeledPoint对象，MLlib通过LabeledPoint来训练模型
  - pos = LabeledPoint(目标, [特征list])
    - 目标值是分类情况 分类值从0开始连续增加
    - 所有特征是double类型

### 使用Spark ML 训练逻辑回归模型

- ①准备数据，准备一个dataframe，所有的特征放到dataframe的一列中，目标放到dataframe的一列中

  ```python
  df3 = VectorAssembler().setInputCols(colArray2).setOutputCol('feautures').transform(df2)
  ```

- ②创建 LogisticRegression对象训练模型

  ```python
  lr = LogisticRegression()
  model = lr.setLabelCol('affairs').setFeaturesCol('feautures').fit(trainDF)
  ```

### CTR预估模型建立

- 利用raw_sample ad_feature user_profile 数据合并 挑选出合适的特征

  ```python
  useful_cols = [
      # 
      # 时间字段，划分训练集和测试集
      "timestamp",
      # label目标值字段
      "clk",  
      # 特征值字段
      "pid_value",       # 资源位的特征向量
      "price",    # 广告价格
      "cms_segid",    # 用户微群ID
      "cms_group_id",    # 用户组ID
      "final_gender_code",    # 用户性别特征，[1,2]
      "age_level",    # 年龄等级，1-
      "shopping_level",
      "occupation",
      "pl_onehot_value",
      "nucl_onehot_value"
  ]
  ```

- 又对数据进行处理，把可能进行one-hot编码的分类特征都进行one_hot处理

  - ```python
    useful_cols_2 = [
        # 时间值，划分训练集和测试集
        "timestamp",
        # label目标值
        "clk",  
        # 特征值
        "price",
        "cms_group_id", #13维
        "final_gender_code", #2维
        "age_level", #7维
        "shopping_level", #3维度
        "occupation", #2
        "pid_value",  #2
        "pl_onehot_value",#4
        "nucl_onehot_value"#5
    ]
    ```

- 逻辑回归训练出的CTR预估模型 预测值的理解

  - 因为数据大部分都是不点击， 样本极度偏斜的，点击样本很少 （5%） 预测出的结果都是0 不点
  - 根据不点击的概率来排序 不点击概率越低的排在前面
  - 在测试数据中 按照不点击的概率排序 考察精准率（找前10个 看看10个中是否有点击的样本）能有10%的CTR预估的概率就已经相当不错了。正常5%。

### 推荐服务

- 离线推荐
  - 先召回对召回结果排序
  - 为每一个用户都进行召回并排序的过程并且把拍好顺序的结果放到数据库中
  - 如果需要推荐结果的时候 直接到数据库中按照user_id查询，返回推荐结果
  - 优点 结构比较简单 推荐服务只需要不断计算，把结果保存到数据库中即可
  - 缺点 实时性差 如果数据1天不更新 1天之内推荐结果一样的，不能反映用户的实时兴趣 
- 实时推荐
  - 排序的模型加载好
  - 召回阶段的结果缓存
  - 所有用户的特征缓存
  - 所有物品的特征缓存
  - 把推荐的服务暴露出去（django flask) 需要推荐结果的服务把 用户id 传递过来
    - 根据id 找到召回结果
    - 根据id 找到缓存的用户特征
    - 根据召回结果的物品id 找到物品的特征
    - 用户特征+物品特征-》逻辑回归模型 就可以预测点击率
    - 所有召回的物品的点记率都预测并排序 推荐topN
    - 实时通过LR模型进行排序的好处
      - 随时修改召回集
      - 随时调整用户的特征
      - 当用户需要推荐服务的时候，获取到最新的召回集和用户特征 得到最新的排序结果 更能体现出用户的实时兴趣

召回（群策群力）

- 协同过滤
- 基于内容召回
- 基于流行度召回

排序（根据自身特征和物品特征预估）

- LR CTR预估



### SparkML 和SparkMLlib 区别

- spark mllib 基于RDD
  - 数据准备 需要创建一个 基于LabeledPoint的RDD
  - LabeledPoint（目标，[特征]）
  - 已经停止更新了 处于维护状态
- spark ML 基于dataframe
  - 数据准备 需要把所有的特征放到一列中 dataframe还需要有一列是 目标值
  - model = lr.setLabelCol('affairs').setFeaturesCol('feautures').fit(trainDF)
  - spark ML 与 sklearn更类似
  - 最新的API放到 spark ML中的

### 缺失值处理

- 分类特征
  - 把缺失作为单独的特征处理
  - 算法预测
- 连续的特征
  - 算法预测
  - 平均值 默认值 中位数填充

### 利用spark 处理 onehot

- 稀疏向量 大部分维度上的值都是0 sparseVector (向量的维度,[非零元素的索引],[非零元素的值])
- 对应的API：stringindexer onehotEncoder pipline 

239146001

